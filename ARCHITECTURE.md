# Architecture

The voice bot is a single-process Python application built around a **bidirectional audio pipeline** connecting Twilio's phone network to an LLM-driven patient simulator. When a call is initiated, Twilio dials the test number and, upon answer, opens a WebSocket Media Stream back to a local FastAPI server exposed via ngrok. Inbound audio (the AI agent's speech) arrives as 8 kHz mu-law frames, forwarded directly to Deepgram's streaming STT service for real-time transcription. The system accumulates Deepgram's final transcripts into a buffer and waits for a 2.5-second silence gap before treating the agent's turn as complete--a value tuned through iteration, since 1.5 seconds caused the bot to interrupt mid-sentence (e.g., cutting off "The first available morning appointment is..." before the time was stated). The accumulated text is then passed to an OpenAI GPT-4o-mini conversation engine that generates a patient response using a scenario-specific persona prompt. That response is synthesized by ElevenLabs using their native `ulaw_8000` output format--eliminating the PCM-to-mulaw conversion step most voice bot architectures require--and streamed back through the Twilio WebSocket in 80 ms frames as chunks arrive, keeping perceived latency low.

Key design choices center on **minimizing latency and maximizing realism while keeping the system simple enough to debug**. GPT-4o-mini was chosen over larger models because patient dialogue needs to be fast and natural, not deeply reasoned--and at 1/10th the cost of GPT-4o it's practical across 12+ calls. ElevenLabs was selected specifically for its native mu-law support, which cut an entire pipeline stage. Scenario parameters are passed to the WebSocket via Twilio's `<Parameter>` TwiML elements rather than query strings, after discovering that Twilio silently strips custom query params from WebSocket connections. The architecture runs a uvicorn server on a background thread while the main thread orchestrates calls sequentially via asyncio, using thread-safe `threading.Event` objects to bridge the two. Post-call analysis is deliberately separated: transcripts are saved as structured JSON and fed through GPT-4o-mini with a QA-focused prompt to produce a categorized bug report, allowing re-analysis without re-calling.
